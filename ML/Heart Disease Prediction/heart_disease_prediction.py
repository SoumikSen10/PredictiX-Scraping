# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bkRCK-Mg7cxIlfOYfgC7UbsdjcT_xXH1

#Importing the dependencies
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""# Data Collection and Processing"""

# Loading the csv data to a Pandas DataFrame
heart_data = pd.read_csv("/content/data.csv")

# Displaying first 5 rows of dataset to observe the structure
heart_data.head()

# Displaying last 5 rows of dataset
heart_data.tail()

# Finding number of rows and columns in dataset
heart_data.shape

# Fetch information about the data
heart_data.info()

# Checking the presence of missing values in dataset
heart_data.isnull().sum()

# Statistical measures of the data
heart_data.describe()

# Checking the distribution of target variable
heart_data['target'].value_counts()

"""*   1 --> Defective Heart
*   0 --> Healthy Heart

# Splitting the Features and Target
"""

# axis = 1 for dropping column and axis = 0 for dropping row
X = heart_data.drop(columns='target',axis=1)
Y = heart_data['target']

# Display training Features data
print(X)

# Display training Target data
print(Y)

"""# Splitting the dataset into training data and testing data"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

# The stratify parameter is used to ensure that the proportion of classes in the train and test sets is the same as in the original dataset. This is particularly
# important when dealing with imbalanced datasets, where some classes are much more frequent than others. By setting stratify=Y, the split will ensure that the class
# distribution in Y_train and Y_test mirrors the distribution in Y.

# The random_state parameter ensures reproducibility of your results. When you set random_state to a specific integer (e.g., random_state=2),
# it ensures that the split you get can be exactly reproduced every time you run the code with that same random state.

# Checking the split
print(X.shape,X_train.shape,X_test.shape)

"""# Model Training"""

# Using Logistic Regression model for binary classification
model = LogisticRegression()

# Training the logistic regression model with training data
model.fit(X_train,Y_train)

"""# Model Evaluation or Testing

Accuracy Score
"""

# Accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction,Y_train)
print("Accuracy on Training Data :",training_data_accuracy)

# Accuracy on testing data
X_test_prediction = model.predict(X_test)
testing_data_accuracy = accuracy_score(X_test_prediction,Y_test)
print("Accuracy on Testing Data :",testing_data_accuracy)

"""# Building the Predictive System"""

input_data = (58,0,0,100,248,0,0,122,0,1,1,0,2)

# Change the input data to a numpy array
input_np_array = np.asarray(input_data)

# Reshaping the numpy array as we are predicting for only one instance
input_data_reshaped = input_np_array.reshape(1,-1)

# Predicting using our reshaped data
prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0] == 0):
 print("The person is not suffering from Heart Disease")
else:
  print("The person is suffering from Heart Disease")